{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPwith_TF_2.0_Text_generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQU7I/zP5z/z0SkfIbDvvx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefan-stein/NLP_with_TF_2.0/blob/master/NLPwith_TF_2_0_Text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZRAv92PWGuy",
        "colab_type": "text"
      },
      "source": [
        "These are my notes for the third part of the \"Natural Language Processing with Tensorflow  2 - Beginner's course\" that can be found [here](https://www.youtube.com/watch?v=B2q5cRJvqI8). This course consists of three parts and the third one deals with text generation. It is based on [this](https://www.tensorflow.org/tutorials/text/text_generation) official tensorflow tutorial. We will create an RNN that creates text one character at a time using Shakespearean text as training data which comes from one of Andrej Karpathy's [blogposts](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMFZQYLcX2Z4",
        "colab_type": "text"
      },
      "source": [
        "# Text generation with RNNs\n",
        "\n",
        "Make sure we are importing tensorflow 2.x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTLwhk8GWWop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc0c8609-6698-445a-c9da-2587319104d5"
      },
      "source": [
        "import os\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWpPG1XCYGb5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca29d73b-6472-4be2-ad4e-d909342fd4c2"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri_y1WVvWk54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the data as txt file\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhzlLvEUYSVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46c6df90-6597-49fa-a262-284457e29fac"
      },
      "source": [
        "# Load the data\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k48VW3FFYoPR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "82998b23-f644-43ab-8276-de244a64fb7b"
      },
      "source": [
        "# Check out what we just downloaded\n",
        "print(text[:250])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t1SykZwZUod",
        "colab_type": "text"
      },
      "source": [
        "We saw above that we have 1.1 million characters. But how bis is our alphabet actually, i.e. how many unique characters do we have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a35BCxbYst8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6446cb2-3bde-4e45-e824-0a23430723a5"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhKeZmxSZ77B",
        "colab_type": "text"
      },
      "source": [
        "Our alphabet consists of 65 unique characters. Next, we create a numeric representation of our alphabet. We create two look-up tables. The first one `char2idx` is simply a dictionary in which each character in our alphabet is given a unique number. The second, `idx2char` is simply a numpy array containing each of the characters in our alphabet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj0HMtVGZrU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_WieCombTpG",
        "colab_type": "text"
      },
      "source": [
        "Now we can create a numeric representation of the entire Shakespeare text, simply by looking up each character in `text` and its associated number in `char2idx`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fp466RaaU9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b7e07a05-8143-40b0-f0d1-2862d1984ecd"
      },
      "source": [
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(text_as_int[:250])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59  1 39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39\n",
            " 58 46 43 56  1 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47\n",
            " 57 46 12  0  0 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53\n",
            " 50 60 43 42  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47\n",
            " 56 57 58  6  1 63 53 59  1 49 52 53 61  1 15 39 47 59 57  1 25 39 56 41\n",
            " 47 59 57  1 47 57  1 41 46 47 43 44  1 43 52 43 51 63  1 58 53  1 58 46\n",
            " 43  1 54 43 53 54 50 43  8  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "parSY3ukbxx5",
        "colab_type": "text"
      },
      "source": [
        "Well, not quite as poetic as its character version, but easier to calculate with. Let's create a prettier representation of what is going on here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKhQNSXYbvUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba599ea5-7a8f-4e7f-c158-9d7950e960a7"
      },
      "source": [
        "print('{} ----> characters mapped to int ----> {}'.format(text[:13], text_as_int[:13]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen ----> characters mapped to int ----> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_GG5O1NdgQs",
        "colab_type": "text"
      },
      "source": [
        "Our approach is as follows: Given an input sequence of characters, say 4 characters, we want to predict the next character. We then add this new predicted character to the sequence and drop the first character in the sequence to form our new input sequence. For example, if we input \"Hell\", a reasonable prediction for the next character would be \"o\". We add that character to the sequence and drop the \"H\", i.e. \"ello\" becomes our new input. We call \"Hell\" the _input_ and \"ello\" the _target_ for our network.\n",
        "\n",
        "Therefore, when we create our training data, we first want to decide on a fixed sequence length (here: `seq_length = 100`) and then slice our data into sequences of `seq_length+1`. In the example above, we would want to create sequences of length 5, such that we can give the first four characters (\"Hell\") to our network and it can learn that the correct continuation of that sequence is \"o\".\n",
        "\n",
        "To achieve this, we first slice our data into single characters using the `from_tensor_slices()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzVvr9gFcXDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "df0bdcda-e325-4d1e-c692-fac0c88a1551"
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g7rvRLKgIdA",
        "colab_type": "text"
      },
      "source": [
        "From here, it is easy to batch the data together to sequences of the desired length using `batch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE2wxUKceeYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e0eabcff-b589-4ac0-997c-567ff3bff906"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DwMwR5sguyi",
        "colab_type": "text"
      },
      "source": [
        "We now create a function to split the data into input and target as described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA5i-fvUefaD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b7c8fc4-dd59-4b35-b758-9c6088d6a9c4"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buM-8f56hwQf",
        "colab_type": "text"
      },
      "source": [
        "Finally, we create training batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBdyALdHhzCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4305d70b-b695-4ceb-a083-625bfde55f66"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKRF2Uawi-AL",
        "colab_type": "text"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "We now are ready to build the model. We use a simple RNN with three layers: An embedding layer with embedding dimension 256 that will map each character in our alphabet to the embedding space. Followed by a recurrent layer and a dense output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9OHTDAfh3T6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LruKhR8xh8U0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIU1pFuOjuDi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "83a2902f-7f15-45bc-ebc5-5936f5882a2d"
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBSvW-JglCUj",
        "colab_type": "text"
      },
      "source": [
        "What does this model architecture actually mean? For each character the model embeds the character with the embedding layer. Then it runs the GRU one timestep with the numeric embedding as input. Then it calculates the probability of each character in our alphabet being the next one in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlOAFsGRj1-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5afaa146-fe8e-47e1-b662-e1e573d93f49"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuymJwLGl9Vw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "62dcf59c-c631-4cf9-cb2e-00cb5a424ba4"
      },
      "source": [
        "example_batch_predictions[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100, 65), dtype=float32, numpy=\n",
              "array([[-2.09618593e-04,  7.35762995e-03,  7.43970321e-03, ...,\n",
              "        -5.30001894e-03, -1.23539604e-02, -1.27230510e-02],\n",
              "       [ 1.23355035e-02, -8.58413428e-03,  1.74405088e-03, ...,\n",
              "        -9.06605041e-04, -1.25144813e-02, -1.74659267e-02],\n",
              "       [ 9.25910287e-03, -8.21050629e-03,  8.07187054e-03, ...,\n",
              "         4.74830903e-03, -1.17486864e-02, -2.36782935e-02],\n",
              "       ...,\n",
              "       [ 1.08108269e-02,  7.79656693e-06, -1.18108317e-02, ...,\n",
              "         2.21678731e-03,  1.64780219e-03, -4.15975507e-03],\n",
              "       [ 8.11548345e-03,  1.33736013e-03,  1.77029450e-03, ...,\n",
              "         6.75642211e-03, -2.33738450e-03, -1.56774353e-02],\n",
              "       [ 1.45033021e-02, -1.72155583e-03,  1.08980108e-04, ...,\n",
              "         4.89278510e-03, -4.40774299e-03,  1.22230286e-02]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBpBeWwZmRAa",
        "colab_type": "text"
      },
      "source": [
        "Above we passed the first input batch to our model (`input_example_batch`). It is of dimension (64,100), i.e. consists of 64 sequences of 100 characters each. When fed into the model, the model returns a tensor of dimension (64, 100, 65). That is, we again have 64 sequences of 100 characters each, but rather than having a fixed character in each position of each sequence, we have a _probability distribution_ over our entire alphabet (notice: the actual numbers in the array are the logits for each character, hence negative values are allowed). Each probability distribution can be represented as a vector of length 65, because our alphabet has 65 unique characters. To actually genereate text from the model, we need to sample from this distribution.\n",
        "\n",
        "Let's try this out. Obviously we are only going to get gibberish for now, since we haven't trained our model properly yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et4ev1Lyl-4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8e48b689-c63e-4640-bc88-ff7f9ac84820"
      },
      "source": [
        "# example_batch_predictions is a collection of 100 probability distributions over our discrete\n",
        "# alphabet of size 65. This line draws one sample from each of these distributions.\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "# The above returned a tensor of shape (100,1). We have to get rid of the (_,1) dimension\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([56, 52, 18, 63, 40,  4, 51, 53, 34, 63, 62, 42, 25, 56, 29, 37, 42,\n",
              "       56, 59, 24, 14, 62, 21, 20, 37, 56,  4,  4, 42, 64, 39, 32, 11, 33,\n",
              "        7, 33, 26, 25,  6, 44, 50, 34, 54, 16, 10, 33, 15, 19, 35,  3, 26,\n",
              "        2, 50,  8, 30, 23, 34, 56, 41, 54,  0, 50, 55, 11, 27, 46,  2, 64,\n",
              "       38, 55, 44, 20, 20, 30, 43, 28, 24, 21, 34, 25, 37, 44, 33, 40, 63,\n",
              "       52,  2, 36, 12,  1,  7, 28, 33, 40, 42, 39, 51, 45, 24, 35])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7QGB93mpSE5",
        "colab_type": "text"
      },
      "source": [
        "`sampled_indices` is _one realization_ of the probability distribution returned by our model for the first input sequence. Let's transform it to characters and see what we have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmsLqC9CmZMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6880c822-538b-486b-d04a-d7ca3318543d"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " \"our rude brawls doth lie a-bleeding;\\nBut I'll amerce you with so strong a fine\\nThat you shall all re\"\n",
            "\n",
            "Next Char Predictions: \n",
            " 'rnFyb&moVyxdMrQYdruLBxIHYr&&dzaT;U-UNM,flVpD:UCGW$N!l.RKVrcp\\nlq;Oh!zZqfHHRePLIVMYfUbyn!X? -PUbdamgLW'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chO6C2U9pqAf",
        "colab_type": "text"
      },
      "source": [
        "This makes no senese at all. Good! This is what we would expect for an untrained network. So let's go ahead and train it.\n",
        "\n",
        "We define our loss and set the `from_logits` flag to `True`, due to the form of the outputs of our network. We try it out using the first target batch (`target_example_batch`) and the batch predictions from above (`example_batch_predictions`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM4SavDyoeiF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b9f14fbd-fcbc-4e7b-f0de-c120ffc1070a"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1745257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbvzJtexsOBh",
        "colab_type": "text"
      },
      "source": [
        "We compile the model and set it up to use the `'adam'` optimizer for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDuyE790sGv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCk3T0zsse6A",
        "colab_type": "text"
      },
      "source": [
        "Next, we configure checkpoints to save training progress after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p73JPg9sslgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s79__3oisodY",
        "colab_type": "text"
      },
      "source": [
        "And we're finally good to go!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGHiVXpcsujv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a4415d0-aeb7-4b91-c6b3-a1869f3e76ab"
      },
      "source": [
        "EPOCHS=30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 172 steps\n",
            "Epoch 1/30\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 2.6809\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 1.9588\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 11s 64ms/step - loss: 1.6944\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 1.5462\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 1.4574\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 1.3987\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 1.3517\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.3130\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.2781\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 1.2454\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 1.2133\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.1812\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 1.1480\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 1.1138\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 1.0794\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 1.0437\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 1.0073\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.9720\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.9359\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.9022\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.8702\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 12s 68ms/step - loss: 0.8405\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.8136\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.7906\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 12s 67ms/step - loss: 0.7688\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 12s 69ms/step - loss: 0.7500\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.7348\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 11s 65ms/step - loss: 0.7194\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 11s 66ms/step - loss: 0.7096\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 11s 67ms/step - loss: 0.6972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aMzY8_Hu_O9",
        "colab_type": "text"
      },
      "source": [
        "Currently this model only accepts batch sizes of 64. To get a prediction for a batch size of one, i.e. for a single sequence, we need to rebuild the model from the last checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jKTXVLdvM3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "395c4a20-9cca-4127-c02d-d6a00efd9a83"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGZCTLuWvRMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWJhaNSSvV9v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "692297b8-993a-497e-de52-2a64657fab16"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qO3uAbQvbX7",
        "colab_type": "text"
      },
      "source": [
        "See? Batch sizes are down to 1 now.\n",
        "\n",
        "So let's generate some text. The following function takes a model and a string to start with. We then convert the `start_string` to its numeric representation. In the `for`-loop below we pass our encoded start string (`input_eval`) into the model. Recall that this returns a collection of probability distributions (as logits) from which we first need to sample to generate actual text. We do this by calling `tf.random.categorical` and store the next predicted character in `predicted_id`. We append this newly predicted character to `input_eval`. No"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPI6or4vvZRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQHReQh_xZuo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "a4613126-821d-4e11-b260-2c6f02863616"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: command men laz.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I know this change, since you can see\n",
            "Be seen it nature to defore this a good nurse. Away with her! Pow Margaret?\n",
            "\n",
            "Pedant:\n",
            "Stir; and, with thy richer-in-law like a drunken sights of stone?\n",
            "\n",
            "WARWICK:\n",
            "Ay, where's my master where no longer speaks,\n",
            "If thou mayst Pilate grapesom of mind.\n",
            "Did I let love I might.\n",
            "\n",
            "ESCALUS:\n",
            "Why, so hath won.\n",
            "\n",
            "LUCIO:\n",
            "Gentle CLorcess\n",
            "Of dambrike the present dreadful steel.\n",
            "That Angelo's as I spice, and thus must I rush'd a word;\n",
            "But, sirrah, or I'll be our kindred's land!\n",
            "Or whether it be, that he is a wife's hate;\n",
            "For in his net nead. Good night.\n",
            "Court, Somerse, when my root with such procect talk'st, revenge furnished: if peachs I till now\n",
            "Can she be full of win:\n",
            "And yet I'll pray thee good, Warwick man or maral world.\n",
            "\n",
            "Third Gentleman:\n",
            "Well, lords, take up, your children yet unboank,\n",
            "And other mother come in's time we see it.\n",
            "\n",
            "GLOUCESTER:\n",
            "He hath a pass in a three-enemies to be straint our hearts,\n",
            "Or an assworn meast is still\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKpnLbYBxim8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}